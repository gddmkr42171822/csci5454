\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage[linesnumbered,lined,boxed,commentsnumbered]{algorithm2e}
\begin{document}
\title{CSCI 5454: PS1}
\author{Robert Werthman}
\date{}
\maketitle

\section*{1.}

\subsection*{}
Let's say these algorithms solve an array sorting problem.\\
\begin{itemize}
\item Let algorithm $A$ be bubblesort with a worst-case runtime of $n^2$.\\
\item Let algorithm $B$ be mergesort with a worst-case runtime of $n*log(n)$.\\
\item Let $C$ be the newly designed sorting algorithm with a worst-case runtime of $h(n)$.\\
\end{itemize}
In this case, $O(min(f(n),g(n)))$ will become $O(n*log(n))$ because it is the smaller of the two runtimes.\\
If $h(n)$ is $log(n)$ then $h(n)$ achieves the running time $O(min(f(n),g(n)))$ because $log(n)$ does not grow faster than $n*log(n)$ and is therefore bounded above by it.\\

\subsection*{}
Yes, you can achieve a running time exactly $min(f(n),g(n))$. Algorithm $C$ would need to be designed in such a way that its running was equal to $min(f(n),g(n))$.\\

\section*{2.}

\subsection*{}
\textbf{Proposition/Claim: } For any real constants $a$ and $b$, where $b > 0$, the asymptotic relation $(n+a)^b = \Theta(n^b)$ is true.\\
\\
\textbf{Theorem: }The asymptotic relation $(n+a)^b = \Theta(n^b)$ is true iff:
\begin{itemize}
\item There exists positive constants $c_1, c_2, n_0$ s
uch that $0 \le c_1(n^b) \le (n+a)^b \le c_2(n^b)$ for all $n \ge n_0$.
\end{itemize}In order to prove the proposition above we must find some constants $c_1, c_2, n_0$ to satisfy the above bulleted sentence.\\
\\
\textbf{Proof: }\\
First we want to find the floor and ceiling of $n+a$ so we can create an inequality similar to the one in the theorem above.
\begin{enumerate}
\item If $|a| \le n$ then we can say that $n+a \le n+|a| \le 2n$ (Ceiling of $n+a$).
\item If $|a| \le \frac{1}{2}n$ then we can say that $n+a \ge n-|a| \ge \frac{1}{2}n$ (Floor of $n+a$). 
\end{enumerate}
Now if $2|a| \le n$ then we can combine the floor and ceilings into an compound inequality that holds true :
$$
0 \le \frac{1}{2}n \le n+a \le 2n
$$
The only thing missing from this new equation is a power of $b$.  Raising the new equation to a power of $b$ gives:
$$
0 \le (\frac{1}{2}n)^b \le (n+a)^b \le (2n)^b \Rightarrow 0 \le (\frac{1}{2})^bn^b \le (n+a)^b \le (2)^bn^b
$$  
Extracting the constants $c_1,c_2,n_0$ from this equation yields $c_1 = (\frac{1}{2})^b$, $c_2 = 2^b$, and $n_0 = 2|a|$ since $n \ge 2|a|$.  These represent one solution.
\section*{3.}
$f(n) = \Omega{g(n)}$ means that for all values to the right of some $n_0$ the value of $f(n)$ is on or above $cg(n)$.\\
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
$n!$&$e^n$&$(\frac{3}{2})^n$&$(lg\,n)!$&$n^2$&$n\,lg\,n$&$lg(n!)$&n&$(\sqrt{2})^{lg\,n}$&$2^{lg*n}$&$n^{1/lg\,n}$&1\\
\hline
\end{tabular}
\end{center}
\subsection*{Equivalence Classes}
$lg(n!) = \Theta(n\,lg\,n)$\\
$n^{1/lg\,n} = \Theta(1)$

\section*{4.}
\subsection*{a.} $T(n) = T(n-1)+n,\,T(1) = 1$\\
I will use a recurrence tree to solve this recurrence relation.\\
\begin{center}
\begin{tikzpicture}
\node (z) {$n$}
child {node (a) {$n-1$} 
child {node (b) {$n-2$}
child {node (c) {$2$}
child {node (d) {$\vdots$}
child {node (e) {$1$}}
}
}
}
};
\end{tikzpicture}
\end{center}
Tree depth = n\\
Cost per level = i\\
So $T(n) = \sum_{i = 1}^{n} i = \frac{n(n+1)}{2} = \frac{n^2}{2} + \frac{n}{2}$\\
Therefore, it can be said that $T(n) =  O(n^2)$\\
\subsection*{b.} $T(n) = 2T(n/2)+n^3,\,T(1) = 1$\\
I will use the master method to solve this recurrence relation.\\
$a=2, b=2, f(n)=n^3$\\
so $n^{\log_{b} a} = n^{\log_{2} 2} = n$\\
This tells us that the first 2 rules of the master theorem do not apply.
\begin{enumerate}
\item $f(n) \ne O(n^{1-\epsilon})$
\item $f(n) \ne \Theta{(n)}$
\end{enumerate}
This leaves the 3rd rule of the master theorem as the solution.\\
\begin{enumerate}
\setcounter{enumi}{2}
\item $f(n) = n^3 = \Omega{(n^{1+\epsilon})}$ if $\epsilon = 1$. \\ 
And $2f(n/2) \le cf(n) \Rightarrow 2(n/2)^3 \le cn^3$ if $c=\frac{1}{2}$ and $n \ge 1$.\\
\end{enumerate}
Therefore, $T(n) = \Theta{(n^3)}$.

\section*{5.}
\subsection*{a.}
\begin{algorithm}
\KwData{Nearly sorted array of size n integers}
\KwResult{Completely sorted array}
\BlankLine
\For {j = 2 to A.length}{
	key = A[j]\;
	i = j - 1\;
	\While {i $>$ 0 and A[i] $>$ key}{
		A[i+1] = A[i]\;
		i = i - 1\;
	}
	A[i+1] = key\;
}
\caption{Insertion-Sort(A)}
\end{algorithm}
\textbf{Analysis: }In order to figure out the running time of Insertion Sort we need to add up the cost of each statement in the algorithm.\\
\begin{itemize}
\item If the array is of size n then the statement \textbf{for j = 2 to A.length} will execute $n$ times with a cost of $c_1$.\\
\item The statements \textbf{key = A[j]} (inserting into an array) and \textbf{i=j-1} (setting a variable) will execute $n-1$ times each with a cost of $c_2$ and $c_3$ respectively.\\
\item Since $k$ elements are unsorted in this array than any unsorted element is no more than $k$ places away from its sorted position.  This means that the statement \textbf{while i $>$ 0 and A[i] $>$ key} could be executed in the worst case $\sum_{j=2}^{n} k$ times with a cost of $c_4$.\\
\item The statements \textbf{A[i+1] = A[i]} (inserting into an array) and \textbf{i = i + 1} (setting a variable) are executed $\sum_{j=2}^{n} k - 1$ times with a cost of $c_5$ and $c_6$ respectively.\\
\item Finally, the statement \textbf{A[i+1] = key} (inserting into an array) is executed $n-1$ times with a cost of $c_7$.\\
\end{itemize}
Therefore, the equation for the runtime, $T(n)$, of insertion-sort is:\\
\begin{align*}
T(n)  & = c_1n + c_2(n-1) + c_3(n-1) + c_4(\sum_{j=2}^{n} k) + c_5(\sum_{j=2}^{n} k - 1) + c_6(\sum_{j=2}^{n} k - 1) + c_7(n-1)\\
	& = c_1n + c_2(n-1) + c_3(n-1) + c_4(k(n-1)) + c_5(\sum_{j=2}^{n} k - 1) + c_6(\sum_{j=2}^{n} k - 1) + c_7(n-1)
\end{align*}
Since $k < n$ further reduction of $T(n)$ would yield a linear function of $n$ so we can say the runtime would turn out to be $O(n)$.

\subsection*{b.}
The sorting algorithm I suggest to get a $O(n)$ runtime is Counting Sort.
\begin{algorithm}
\KwData{A is the input array of length $n$}
\KwData{B is the sorted array of length $n$}
\KwData{$k$ is the highest integer in A}
\BlankLine
let $C[0..k]$ be a new array\\
\For{$i=0$ to $k$}{
	$C[i] = 0$\\
}
\For{$j=1$ to $A.length$}{
	$C[A[j]] = C[A[j]+1$\\
}
\For{$i=1$ to $k$}{
	$C[A[j]] = C[i] + C[i-1]$\\
}
\For{$j = A.length$ downto 1}{
	$B[C[A[j]]] = A[j]$\\
	$C[A[j]] = C[A[j]] - 1$\\
}
\caption{Counting-Sort(A, B, $k$)}
\end{algorithm}
\\
\textbf{Analysis: }
\begin{itemize}
\item Initializing $C[0..k]$ takes $k+1$ time to execute and costs $c_0$.
\item The statement \textbf{for i = 0 to k} takes $k+1$ times to execute and costs $c_1$.
\item The statements \textbf{for j = 1 to A.length} and \textbf{j = A.length downto 1} take $n$ times to execute and cost $c_3$ and $c_4$ respectively.
\item The statement \textbf{i = 1 to k} takes $k$ times to execute and costs $c_2$.
\end{itemize}
The equation for the runtime, $T(n)$, of Counting Sort is:\\
$$
T(n) = c_0(k+1) + c_1(k+1) + c_3n + c_4n + c_2k\ldots
$$

\noindent
Reducing $T(n)$ further would show that the runtime of Counting Sort is a linear function of $n$ that runs in a linear time of $O(k+n)$.
If $k=O(n)$ then the running time is $\Theta(n)$.

\subsection*{c.}
(b) doesn't contradict the $\Omega{(n\,log\,n)}$ lower bound given on page 59 of the textbook because the algorithm is not a comparison sorting algorithm.\\
It has been proven that any comparison sort must make $\Omega{n\,log\,n)}$ comparisons in the worst case to sort $n$ elements.  Since counting sort is not a comparison sorting algorithm its runtime is not bounded by $\Omega{(n\,log\,n)}$.

\section*{6}
\textbf{Lemma 1: }  A good minion tells the truth.\\
\textbf{Lemma 2: } A bad minion could be telling the truth or could be lying.\\
\\
Let $g$ be the number of good minions, $b$ be the number of bad minions, and $n$ be the total number of minions.\\
\\
\subsection*{a.}
\textbf{Proposition/Claim :} 
If $n/2$ or more minions are bad, Gru cannot necessarily determine which minions are good.\\
\\
\textbf{Proof: }  This claim is proven by analyzing the cases.\\
\\
The comparison in the chamber can be between two good minions, two bad minions, or one good and one bad minion.\\
\\
The claim assumes that $b \ge n/2$.\\
\\
\textbf{Case 1: } Two good minions size each other up.\\
The result of that comparison would be:\\
\begin{center}
\begin{tabular}{c|c}
Minion A & Minion B\\
\hline
good & good\\
\end{tabular}
\end{center}
\textbf{Case 2: } Two bad minions size each other up.\\
The result of the comparison could be:
\begin{center}
\begin{tabular}{c|c}
Minion A & Minion B\\
\hline
good & good\\
bad & bad\\
good & bad\\
bad & good\\
\end{tabular}
\end{center}
\textbf{Case 3: } One good minion and one bad minion size each other up.\\
The result of the comparison if A was good and B was bad could be: 
\begin{center}
\begin{tabular}{c|c}
Minion A & Minion B\\
\hline
good & bad\\
bad & bad\\
\end{tabular}
\end{center}
The result of the comparison if A was bad and B was good could be: 

\begin{center}
\begin{tabular}{c|c}
Minion A & Minion B\\
\hline
bad & good\\
bad & bad\\
\end{tabular}
\end{center}

\noindent
\textbf{Analysis: }As can be seen from the cases above, two bad minions sizing each other up in the chamber 
can lead to the same results as two good minions or one good minion and one bad minion sizing each other up in the chamber.
\\
Gru has no way to tell if the results he is seeing in the chamber are from two good minions, two bad minions, or one bad and one good minion. 

\subsection*{b.}
\textbf{Proposition/Claim :} 
$n/2$ pairwise tests are sufficient to reduce the problem of finding a single good minion to one of nearly half the size.\\
\\
The claim assumes that $g > n/2$.\\
\\
\textbf{Proof: }This claim is proven by analyzing the cases.\\

\end{document}  